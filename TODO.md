# TODO 

## Algorithmic changes

### 0. Make Neuronal Nets smaller

### 1. Improve DQN

- Experience replay 
- Add buffer size to include about 500 episodes 
- Double DQN 
- Prevent overfitting (reduce first layer to 32 neurons) 
- Test MÃ¼nchhausener RL 

### 2. Use entropy instead of uncertainty 

### 3. How to create fair rewards to reach certain goals 

## Papers 

https://arxiv.org/abs/2007.14430 

https://arxiv.org/pdf/1702.03037.pdf



## Feedback 05.01

### DQN / MDQN 

- Explore longer (epsilon decay higher)
- Reduce learning rate 
- 

### Ideas (Fabian)

- Potential based learning (Potential shaped rewards)
- Learn tasks subseqential 
- Start with small state and fill missing values with 0 
- Extend state on the fly 
- Improve state (give cab only a flag for passenger)
- QMixing / VDQN
- Choices: Micro vs Makro


### Own new ideas

- create more specific goals 
- Lower driving speed -> lower consumption

